# -*- coding: utf-8 -*-
"""Copy of Moringa_Data_Science_Prep_W3_Independent_Project_2021_08_Moreen_Mugambi_python_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18w1eT6yhm5gulmj8xuI-7-JDrk22VoBe
"""

# importing the libraries
import pandas as pd
import numpy as np

# loading the datasets, csv files

df1 = pd.read_csv('Telcom_dataset.csv', delimiter = ',')
df1

# then we start cleaning the data
# change column names to be same in all data sets

df1.rename(columns= {'PRODUTC': 'PRODUCT', 'DATETIME': 'DATE_TIME'}, inplace = True)
df1

# drop country a and b and cell on site
df1.drop(['COUNTRY_A', 'COUNTRY_B', 'CELL_ON_SITE'],  axis=1, inplace=True)
df1

# drop duplicates
df1.drop_duplicates()

df1.head()

# loading dataset, csv file

df2 = pd.read_csv('Telcom_dataset2.csv', delimiter = ',')
df2

# clean data
# change column names to match names in all data sets
df2.rename(columns= {'DW_A_NUMBER':'DW_A_NUMBER_INT', 'DW_B_NUMBER':'DW_B_NUMBER_INT'},inplace = True)
df2

#drop country a and b and cell on site
df2.drop(['COUNTRY_A', 'COUNTRY_B','CELL_ON_SITE'], axis = 1, inplace=True)
df2

# drop duplicates
df2.drop_duplicates()

df2.head()

# loading dataset, csv file
df3 = pd.read_csv('Telcom_dataset3.csv')
df3

# clean data
# change column names to match names in all data sets
df3.rename(columns= {'SIET_ID':'SITE_ID', 'CELLID': 'CELL_ID'}, inplace =  True)
df3

# drop country a and b and cell on site
df3.drop(['COUNTRY_A', 'COUNTRY_B', 'CELL_ON_SITE'], axis = 1, inplace= True)
df3

# drop duplicates
df3.drop_duplicates()

df3.head()

def check_data(dataset):
  print(dataset.info())
  print("The shape of the dataset is ", dataset.shape)
  print("Number of duplicates is ", dataset.duplicated().sum())
  print("The columns in the dataset are ", dataset.columns)
  print("Number of missing values per column is ", dataset.isnull().sum())

check_data(df1)

check_data(df2)

check_data(df2)

# merging the three data frames
df_1_2_3 = pd.concat([df1, df2,df3], ignore_index= True)
df_1_2_3.head(3)

# split date and time columns 
df_new = df_1_2_3["DATE_TIME"].str.split(" ", n = 1, expand = True)
  
# making separate date column from new data frame 
df_1_2_3["DATE"] = df_new[0] 
  
# making separate time column from new dataset
df_1_2_3["TIME"] = df_new[1] 

df_1_2_3.head(3)

# Dropping DATE_TIME column 
df_1_2_3.drop(['DATE_TIME'], axis = 1, inplace = True) 
df_1_2_3.head(3)

# loading the geo csv file
geo = pd.read_csv('cells_geo.csv', sep = ";")
geo.head()

check_data(geo)

"""There are no duplicates in this data.
Status column has 67 missing values.
Area column has 23 missing values.
"""

# drop null values according to the rows

new_geo = geo.dropna()
new_geo.head(3)

# let us drop some columns

new_geo.drop(['STATUS', 'Unnamed: 0', 'ZONENAME','LONGITUDE', 'LATITUDE', 'DECOUPZONE'], axis = 1, inplace = True) 
new_geo.head(3)

# lets merge new geo and telcom datasets
new_d = pd.concat([new_geo,df_1_2_3 ], ignore_index= True)
new_d.head(3)

# replace null values with missing
new_d.replace(to_replace= np.nan, value = 'missing').head(3)

#new_d.groupby(['PRODUCT', 'VILLES','VALUE'])
N = new_d.sort_values(by=['VILLES', 'PRODUCT', 'VALUE', 'TIME'])
N2 = N.groupby('PRODUCT').count()
N2

# most used product
new_d.groupby(['PRODUCT'])[['VALUE']].sum()

# viewing our data
new_d['VALUE'].groupby(new_d['PRODUCT']).describe()

# skewness
new_d['VALUE'].groupby(new_d['PRODUCT']).skew()

#
new_d['VALUE'].groupby(new_d['PRODUCT']).std()

# correlation matrix of values
new_d.corr()

# to figure out the most used city for the three days
new_d['VILLES'].value_counts()